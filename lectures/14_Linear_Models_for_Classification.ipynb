{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14_Linear_Models_for_Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPI0jnhGygoC"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression, Perceptron\n",
        "from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRFwrtvRxe2i"
      },
      "source": [
        "# Linear Models for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcE0yTNSxkxm"
      },
      "source": [
        "## Dataset: BACE-1\n",
        "\n",
        "Beta-Secretase 1 (BACE) is a transmembrane aspartic-acid protease human protein encoded by the BACE1 gene. BACE is essential for the generation of beta-amyloid peptide in neural tissue, a component of amyloid plaques widely believed to be critical in the development of Alzheimer's, rendering BACE an attractive therapeutic target. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlfLtftFxdvn"
      },
      "source": [
        "os.system(\"wget https://raw.githubusercontent.com/deepchem/deepchem/master/datasets/desc_canvas_aug30.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh8CgCrc2M2_"
      },
      "source": [
        "This dataset contains a set of molecular structures `mol`, half-maximal inhibitory concentration `pIC50`, and 590 molecular topological features. These features can be calculated using common chemistry Python packages like `openbabel` or `rdkit`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7se9fPzzCv4"
      },
      "source": [
        "df = pd.read_csv(\"desc_canvas_aug30.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWmETP_d24Tc"
      },
      "source": [
        "This dataset was previous used in a drug design competition sponsored by Novartis. Here, we use the original train/test dataset splitting of the contest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_37WS53w0kvk"
      },
      "source": [
        "train_df = df[df['Model'] == \"Train\"]\n",
        "test_df = df[df['Model'] == \"Test\"]\n",
        "\n",
        "label = 'Class'\n",
        "y_train = train_df[label].values\n",
        "y_test = test_df[label].values\n",
        "\n",
        "features = list(train_df.keys()[5:-1])\n",
        "features = [f for f in features if not np.isnan(np.sum(train_df[f].values))]\n",
        "X_train = train_df[features].values\n",
        "X_test = test_df[features].values\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNzdEIrJ3Kua"
      },
      "source": [
        "## Linear Models for Classification\n",
        "\n",
        "The goal in classification is to take an input vector $\\textbf{x}$ and to assign it to one of K discrete classes $\\mathcal{C}_k$ where k = 1,..,K. In the most common scenario, the classes are considered to be disjoint, such that each example is assigned to a single class. The input space is then divided into decision regions whose boundaries are class decision boundaries. \n",
        "\n",
        "Today, we will consider linear models for classification, by which we mean that the decision boundaries are linear functions of the input vector $\\textbf{x}$. The resulting decision surfaces are defined by (D-1)-dimensional hyperplances within a D-dimensional input space.  \n",
        "\n",
        "<center><img src=\"https://i.imgur.com/hNAusMw.png\" height=400>\n",
        "\n",
        "Example of a linearly seperable decision problem (Rep. Bishop Fig 4.4a) </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTNNxRBoLzBS"
      },
      "source": [
        "## Logistic Regression\n",
        "\n",
        "In logistic regression, we fit a model of the form:\n",
        "\n",
        "$$ y(\\textbf{x}, \\textbf{w}) = \\sum_{j=0}^{M-1} w_j \\phi_j(\\textbf{x}) = \\textbf{w}^T \\boldsymbol{\\phi}(\\textbf{x}) $$\n",
        "\n",
        "We assume a linear relationship between predictor variables $\\textbf{x}$ and the log-odds that the example belongs to some class $\\mathcal{C}_k$:\n",
        "\n",
        "$$ log \\frac{p}{1-p} = \\textbf{w}^T \\boldsymbol{\\phi}(\\textbf{x}) $$\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/t7rhx63.png\" height=400>\n",
        "\n",
        "Example logistic regression (Rep. Bishop Fig 4.1)\n",
        "</center>\n",
        "\n",
        "The weights $\\textbf{w}$ are obtained by maximizing the log-likelihood of the model given the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbikda0O_Q4M"
      },
      "source": [
        "# C is the inverse regularization strength i.e. 1/alpha\n",
        "reg = LogisticRegression(penalty='l1', C=10.0, solver='liblinear')\n",
        "reg.fit(X_train, y_train)\n",
        "print(reg.score(X_train, y_train))\n",
        "print(reg.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUiWMO4zPDHO"
      },
      "source": [
        "## Exercise: Fit a logistic regression model using the larger \"test\" dataset and evaluate model performance on the smaller \"train\" dataset. What is the effect of tuning \"C\"? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl57v0xZPC1t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsMQsmHb3ODI"
      },
      "source": [
        "## Perceptron\n",
        "\n",
        "We consider a non-linear modification of the logistic regression model, the perceptron. The functional form of the perceptron model is:\n",
        "\n",
        "$$ y(\\textbf{x}, \\textbf{w}) = \\mathcal{f}(\\textbf{w}^T \\mathbb{\\phi}(\\textbf{x})) $$\n",
        "\n",
        "where f is a step function of the form\n",
        "\n",
        "$$ f(a) = \\begin{cases} \n",
        "      +1 & a\\geq 0 \\\\\n",
        "      -1 & a \\lt 0 \n",
        "   \\end{cases}\n",
        "$$\n",
        "\n",
        "The perceptron error function is given by:\n",
        "\n",
        "$$ E(\\textbf{w}) = - \\sum_{n \\in M} \\textbf{w}^T \\mathbb{\\phi}(\\textbf{x}_n) t_n $$\n",
        "\n",
        "This error function is typically optimized through stochastic gradient descent.\n",
        "\n",
        "The perceptron famously is unable to solve the XOR problem, a simple two-variable single-class classification problem. However, it was shown that stacking/nesting perceptrons into what is refered to as a multi-layer perceptron (or neural network) can allieviate this issue.\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/LZNuV4D.png\" height=400>\n",
        "\n",
        "Rep. Bishop Fig. 4.8 </center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1qFX_gn_ssX"
      },
      "source": [
        "reg = Perceptron(penalty='l2', alpha=0.11)\n",
        "reg.fit(X_test, y_test)\n",
        "print(reg.score(X_train, y_train))\n",
        "print(reg.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvXkWTC4D9Uq"
      },
      "source": [
        "## Exercise: The XOR problem\n",
        "\n",
        "Attempt to train a perceptron using the XOR dataset below. What is the predicted accuracy score? What examples are being classified incorrectly?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFV1VOVkEBMA"
      },
      "source": [
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])\n",
        "reg = Perceptron()\n",
        "reg.fit(X, y)\n",
        "print(reg.score(X, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xYg301a3To6"
      },
      "source": [
        "## Classification Metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2e9f4OnYPjN"
      },
      "source": [
        "reg = LogisticRegression(penalty='l1', C=10.0, solver='liblinear')\n",
        "reg.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAfWgV5aYFTp"
      },
      "source": [
        "Accuracy Score: ratio of (true positives + true negatives) to (true positives + true negatives + false positives + false negatives)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_sBwuOICpjH"
      },
      "source": [
        "print(accuracy_score(reg.predict(X_train), y_train))\n",
        "print(accuracy_score(reg.predict(X_test), y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dqd3wOdYfRM"
      },
      "source": [
        "Receiver Operating Characteristic (ROC) Curve: A plot showing the true positive rate versus the false positive rate as the discriminative threshold is varied"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB0tLyTEZOPO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fpr, tpr, thresholds = roc_curve(y_test, reg.predict_proba(X_test)[:, 1])\n",
        "plt.plot(fpr, tpr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6DZtuYvZOul"
      },
      "source": [
        "Area Under the Receiver Operating Characteristic Curve (ROC-AUC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQNW_lpgZbT-"
      },
      "source": [
        "print(roc_auc_score(y_train, reg.predict_proba(X_train)[:, 1]))\n",
        "print(roc_auc_score(y_test, reg.predict_proba(X_test)[:, 1]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}