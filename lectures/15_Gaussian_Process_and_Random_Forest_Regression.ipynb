{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gaussian Process and Random Forest Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GY-XeXJU5CZ"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer\n",
        "from sklearn.model_selection import cross_validate, train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTm3ofQJU7t8"
      },
      "source": [
        "# Gaussian Process Regression for Time Series Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct2P9MhdVBYG"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels \\\n",
        "    import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY3VRlRMuMMk"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "This example is based on Section 5.4.3 of “Gaussian Processes for Machine Learning”. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to model the CO2 concentration as a function of the time t."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAIw-gyyY5Dc"
      },
      "source": [
        "def load_mauna_loa_atmospheric_co2():\n",
        "    ml_data = fetch_openml(data_id=41187)\n",
        "    months = []\n",
        "    ppmv_sums = []\n",
        "    counts = []\n",
        "\n",
        "    y = ml_data.data[:, 0]\n",
        "    m = ml_data.data[:, 1]\n",
        "    month_float = y + (m - 1) / 12\n",
        "    ppmvs = ml_data.target\n",
        "\n",
        "    for month, ppmv in zip(month_float, ppmvs):\n",
        "        if not months or month != months[-1]:\n",
        "            months.append(month)\n",
        "            ppmv_sums.append(ppmv)\n",
        "            counts.append(1)\n",
        "        else:\n",
        "            # aggregate monthly sum to produce average\n",
        "            ppmv_sums[-1] += ppmv\n",
        "            counts[-1] += 1\n",
        "\n",
        "    months = np.asarray(months).reshape(-1, 1)\n",
        "    avg_ppmvs = np.asarray(ppmv_sums) / counts\n",
        "    return months, avg_ppmvs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OKiishvY5H3"
      },
      "source": [
        "X, y = load_mauna_loa_atmospheric_co2()\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vllq-BIcubjD"
      },
      "source": [
        "## Split Dataset\n",
        "\n",
        "Here, we train on the first 80% of the time series and predict the future 20% (temporal split)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhYNmBMgiDj0"
      },
      "source": [
        "n_examples = y.shape[0]\n",
        "train_idx = int(0.8 * n_examples)\n",
        "\n",
        "X_train = X[:train_idx]\n",
        "y_train = y[:train_idx]\n",
        "X_test = X[train_idx:]\n",
        "y_test = y[train_idx:]\n",
        "print(X_train.shape, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK5dpu8aukG0"
      },
      "source": [
        "## Kernel Construction\n",
        "\n",
        "The kernel is composed of several terms that are responsible for explaining different properties of the signal:\n",
        "\n",
        "- a long term, smooth rising trend is to be explained by an RBF kernel. The RBF kernel with a large length-scale enforces this component to be smooth; it is not enforced that the trend is rising which leaves this choice to the GP. The specific length-scale and the amplitude are free hyperparameters.\n",
        "\n",
        "- a seasonal component, which is to be explained by the periodic ExpSineSquared kernel with a fixed periodicity of 1 year. The length-scale of this periodic component, controlling its smoothness, is a free parameter. In order to allow decaying away from exact periodicity, the product with an RBF kernel is taken. The length-scale of this RBF component controls the decay time and is a further free parameter.\n",
        "\n",
        "- smaller, medium term irregularities are to be explained by a RationalQuadratic kernel component, whose length-scale and alpha parameter, which determines the diffuseness of the length-scales, are to be determined. According to [RW2006], these irregularities can better be explained by a RationalQuadratic than an RBF kernel component, probably because it can accommodate several length-scales.\n",
        "\n",
        "- a “noise” term, consisting of an RBF kernel contribution, which shall explain the correlated noise components such as local weather phenomena, and a WhiteKernel contribution for the white noise. The relative amplitudes and the RBF’s length scale are further free parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kceNG2s1iTyM"
      },
      "source": [
        "k1 = 50.0**2 * RBF(length_scale=50.0)  # long term smooth rising trend\n",
        "k2 = 2.0**2 * RBF(length_scale=100.0) \\\n",
        "    * ExpSineSquared(length_scale=1.0, periodicity=1.0,\n",
        "                     periodicity_bounds=\"fixed\")  # seasonal component\n",
        "# medium term irregularities\n",
        "k3 = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
        "k4 = 0.1**2 * RBF(length_scale=0.1) \\\n",
        "    + WhiteKernel(noise_level=0.1**2,\n",
        "                  noise_level_bounds=(1e-3, np.inf))  # noise terms\n",
        "kernel = k1 + k2 + k3 + k4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSwoIirkuxqn"
      },
      "source": [
        "# Train model\n",
        "\n",
        "We optimize the kernel hyperparameters by maximizing the model log likelihood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrZDZ24FiVXu"
      },
      "source": [
        "gp = GaussianProcessRegressor(kernel=kernel, alpha=0,\n",
        "                              normalize_y=True)\n",
        "gp.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPpqqkmru8Wl"
      },
      "source": [
        "## Evaluate model\n",
        "\n",
        "We can evaluate our model by computing the R2 score and mean absolute error on the train and test set. We then plot our model forecast with the true measurements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-9vyIJio4RU"
      },
      "source": [
        "y_train_pred = gp.predict(X_train)\n",
        "y_test_pred = gp.predict(X_test)\n",
        "print(\"train r2_score:\", r2_score(y_train, y_train_pred), \"train mae: \", mean_absolute_error(y_train, y_train_pred))\n",
        "print(\"test r2_score:\", r2_score(y_test, y_test_pred), \"test mae: \", mean_absolute_error(y_test, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wboeJBfvid3U"
      },
      "source": [
        "X_ = np.linspace(X.min(), X.max(), 1000)[:, np.newaxis]\n",
        "y_pred, y_std = gp.predict(X_, return_std=True)\n",
        "\n",
        "# Illustration\n",
        "plt.plot(X, y, c='r')\n",
        "plt.plot(X_, y_pred)\n",
        "plt.fill_between(X_[:, 0], y_pred - y_std, y_pred + y_std,\n",
        "                 alpha=0.5, color='k')\n",
        "plt.xlim(X_.min(), X_.max())\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(r\"CO$_2$ in ppm\")\n",
        "plt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lKXfMA6vMuz"
      },
      "source": [
        "## Re-train Model for Prospective Use\n",
        "\n",
        "We can retrain the model with the entire dataset and then use the retrained model to predict out to 2030. Note how the uncertainty (gray envelope) increases as we project out further in time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1__lFJ4Y5Mw"
      },
      "source": [
        "k1 = 50.0**2 * RBF(length_scale=50.0)  # long term smooth rising trend\n",
        "k2 = 2.0**2 * RBF(length_scale=100.0) \\\n",
        "    * ExpSineSquared(length_scale=1.0, periodicity=1.0,\n",
        "                     periodicity_bounds=\"fixed\")  # seasonal component\n",
        "# medium term irregularities\n",
        "k3 = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
        "k4 = 0.1**2 * RBF(length_scale=0.1) \\\n",
        "    + WhiteKernel(noise_level=0.1**2,\n",
        "                  noise_level_bounds=(1e-3, np.inf))  # noise terms\n",
        "kernel = k1 + k2 + k3 + k4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEuB0QRgY5Qn"
      },
      "source": [
        "gp = GaussianProcessRegressor(kernel=kernel, alpha=0,\n",
        "                              normalize_y=True)\n",
        "gp.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjjgShscY5WI"
      },
      "source": [
        "print(\"\\nLearned kernel: %s\" % gp.kernel_)\n",
        "print(\"Log-marginal-likelihood: %.3f\"\n",
        "      % gp.log_marginal_likelihood(gp.kernel_.theta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S97sLjMSZytq"
      },
      "source": [
        "X_ = np.linspace(X.min(), X.max() + 30, 1000)[:, np.newaxis]\n",
        "y_pred, y_std = gp.predict(X_, return_std=True)\n",
        "\n",
        "# Illustration\n",
        "plt.scatter(X, y, c='k')\n",
        "plt.plot(X_, y_pred)\n",
        "plt.fill_between(X_[:, 0], y_pred - y_std, y_pred + y_std,\n",
        "                 alpha=0.5, color='k')\n",
        "plt.xlim(X_.min(), X_.max())\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(r\"CO$_2$ in ppm\")\n",
        "plt.title(r\"Atmospheric CO$_2$ concentration at Mauna Loa\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB4o4ONuVCda"
      },
      "source": [
        "# Random Forest Regression for Protein-Ligand Binding Energy Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdPovlVhvmbR"
      },
      "source": [
        "## Software installation\n",
        "\n",
        "Although Google Colab comes with many useful numerical libraries pre-installed, other libraries require external installation. Here, we install the `rdkit` open source cheminformatics packages for performing feature engineering with the BACE dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed1kDIR7bOog"
      },
      "source": [
        "!wget -c https://repo.continuum.io/miniconda/Miniconda3-py37_4.8.3-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-py37_4.8.3-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-py37_4.8.3-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!time conda install -q -y -c conda-forge rdkit\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uG8axQxbaTO"
      },
      "source": [
        "from scipy import stats\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDicT1BOwDM4"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "Let's load up the BACE dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxEnvqd-dkYo"
      },
      "source": [
        "os.system(\"wget https://raw.githubusercontent.com/deepchem/deepchem/master/datasets/desc_canvas_aug30.csv\")\n",
        "df = pd.read_csv(\"desc_canvas_aug30.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX7scM2Lciot"
      },
      "source": [
        "label = 'pIC50'\n",
        "y = df[label].values\n",
        "smiles = df['mol'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsgJIv2wi5h"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "We compute the topological fingerprint of each molecule in the dataset using the Extended Connectivity Fingerprint (ECFP) algorithm.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1180/1*oaJ6HRYeCImh7TmMrS345Q.png\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuk-uBArczMD"
      },
      "source": [
        "fps = []\n",
        "for s in smiles:\n",
        "  m = Chem.MolFromSmiles(s)\n",
        "  fp = AllChem.GetMorganFingerprintAsBitVect(m, 2, nBits=768)\n",
        "  fps.append(fp)\n",
        "X = np.stack(fps, axis=0)\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqhB6DbSxbPg"
      },
      "source": [
        "## Dataset Splitting and Transformation\n",
        "\n",
        "We split the dataset into 80% training and 20% testing. We standardize the pIC50 values using the mean and stdev from the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRK_A4Agd9Jx"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
        "\n",
        "y_mean = y_train.mean()\n",
        "y_std = y_train.std()\n",
        "\n",
        "y_train_t = (y_train - y_mean) / y_std\n",
        "y_test_t = (y_test - y_mean) / y_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMIeEtLcxo3U"
      },
      "source": [
        "## Cross-validation\n",
        "\n",
        "We perform 5-fold cross-validation using the training set. Here, we see how the model improves by adding more estimators or \"trees\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3ZkFQu0g8NG"
      },
      "source": [
        "scoring = {'r2_score': make_scorer(r2_score)}\n",
        "for est in [10, 20, 40, 80, 160]:\n",
        "  cv = cross_validate(RandomForestRegressor(n_estimators=est, criterion='mse', max_features=0.33), \n",
        "                      X_train, y_train_t, scoring=scoring, return_train_score=True)\n",
        "  for score in ['train_r2_score', 'test_r2_score']:\n",
        "    scores = cv[score]\n",
        "    mean_scores = np.round(np.mean(scores), 4)\n",
        "    stderr_scores = np.round(stats.sem(scores), 4)\n",
        "    print(\"n_estimators: {} {}: {} +/- {}\".format(est, score, mean_scores, stderr_scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98uff26yx2Or"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "We evaluate the final model using the train and (unseen) test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj3FU9d8i_gI"
      },
      "source": [
        "reg = RandomForestRegressor(n_estimators=160, criterion='mse', max_features=0.33)\n",
        "reg.fit(X_train, y_train_t)\n",
        "print(reg.score(X_train, y_train_t))\n",
        "print(reg.score(X_test, y_test_t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbAxFHqmkDRt"
      },
      "source": [
        "y_pred_train = reg.predict(X_train)*y_std + y_mean\n",
        "y_pred_test = reg.predict(X_test)*y_std + y_mean\n",
        "print(\"train r2:\", r2_score(y_train, y_pred_train), \"train mae:\", mean_absolute_error(y_train, y_pred_train))\n",
        "print(\"test r2:\", r2_score(y_test, y_pred_test), \"test mae:\", mean_absolute_error(y_test, y_pred_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKuke8YYkfRQ"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(15, 7.5))\n",
        "ax[0].scatter(y_train, y_pred_train)\n",
        "ax[0].plot(np.linspace(0, 10), np.linspace(0, 10), 'r')\n",
        "ax[0].set_aspect('equal', adjustable='box')\n",
        "ax[0].set_xlabel(\"Log Solubility\")\n",
        "ax[0].set_ylabel(\"Predicted Log Solubility\")\n",
        "ax[0].set_title(\"Train R2: {}\".format(r2_score(y_train, y_pred_train)))\n",
        "ax[1].scatter(y_test, y_pred_test)\n",
        "ax[1].plot(np.linspace(0, 10), np.linspace(0, 10), 'r')\n",
        "ax[1].set_aspect('equal', adjustable='box')\n",
        "ax[1].set_xlabel(\"Log Solubility\")\n",
        "ax[1].set_ylabel(\"Predicted Log Solubility\")\n",
        "ax[1].set_title(\"Test R2: {}\".format(r2_score(y_test, y_pred_test)))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSocGf1WwMH9"
      },
      "source": [
        "## Visualizing Feature Importance\n",
        "\n",
        "We can use the computed feature importances from the `RandomForestRegressor` and visualize the corresponding molecular substructors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSNdyp8-qYT3"
      },
      "source": [
        "fi = np.array(reg.feature_importances_)\n",
        "sortidx = np.argsort(fi)\n",
        "print(fi[sortidx[-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctDsT-uBqbmO"
      },
      "source": [
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem import Draw\n",
        "bi = {}\n",
        "m = Chem.MolFromSmiles(smiles[1], sanitize=False)\n",
        "Chem.SanitizeMol(m,sanitizeOps=Chem.SanitizeFlags.SANITIZE_ALL^Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
        "m.UpdatePropertyCache()\n",
        "fp = AllChem.GetMorganFingerprintAsBitVect(m, 2, nBits=768, bitInfo=bi)\n",
        "m"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIBgePMpsXs7"
      },
      "source": [
        "mfp = np.array(fp)\n",
        "sorted_bits = []\n",
        "for i in sortidx[::-1]:\n",
        "  if mfp[i] == 1: sorted_bits.append(i)\n",
        "print(sorted_bits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QH7LySjrOLb"
      },
      "source": [
        "Draw.DrawMorganBit(m, sorted_bits[1], bi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMMAMn9jrdD4"
      },
      "source": [
        "Draw.DrawMorganBit(m, sorted_bits[2], bi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWYbXP-XrlhO"
      },
      "source": [
        "Draw.DrawMorganBit(m, sorted_bits[4], bi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sbZKWMqt2w7"
      },
      "source": [
        "Draw.DrawMorganBit(m, sorted_bits[5], bi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKOczegyuEOI"
      },
      "source": [
        "Draw.DrawMorganBit(m, sorted_bits[7], bi)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}