{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwcM_hhjabYS"
      },
      "source": [
        "!pip install skorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5801kqPw-lK"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import skorch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSmF56qJ2q1-"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDpNlSwR-hT2"
      },
      "source": [
        "## 2-D Convolution\n",
        "\n",
        "Important parameters:\n",
        "- `in_channels`: number of entering \"features\" e.g. color channels, data dimension\n",
        "- `out_channels`: number of outgoing features\n",
        "- `kernel_size`: receptive field of convolutional operator\n",
        "- `stride`: step size for of convolutional operator\n",
        "- `padding`: image padding to be applied before convolutional operator\n",
        "\n",
        "How it works:\n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif'>\n",
        "\n",
        "ref: <a href='https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif'>Wikimedia Commons</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5k096Fg_hw9"
      },
      "source": [
        "## Example: Cat Laplacian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxBasExY_kIo"
      },
      "source": [
        "os.system(\"wget https://upload.wikimedia.org/wikipedia/commons/3/3c/IC_Blue_Melody_Flipper_CHA_male_EX1_CACIB.jpg\")\n",
        "\n",
        "def image_loader(path):\n",
        "\n",
        "  imsize = 256\n",
        "  # Image loading\n",
        "  # 1. Resize image so that smaller dim = imsize\n",
        "  # 2. Center crop image along larger dimension to make imsize x imsize\n",
        "  # 3. Convert to Pytorch tensor\n",
        "  loader = transforms.Compose([transforms.Resize(imsize), \n",
        "                               transforms.CenterCrop(imsize),\n",
        "                               transforms.ToTensor()])\n",
        "  img = Image.open(path)\n",
        "  img = loader(img).requires_grad_(True).unsqueeze(0)\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9bnfwZ50MF9"
      },
      "source": [
        "img = image_loader('IC_Blue_Melody_Flipper_CHA_male_EX1_CACIB.jpg')\n",
        "img_py = transforms.ToPILImage()(img[0])\n",
        "display(img_py)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsbR_tF0zjSp"
      },
      "source": [
        "# initialize a Conv2d object for the Laplacian operator\n",
        "lapl = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, bias=False)\n",
        "\n",
        "# set the weights of the Conv2d to match the Laplacian operator\n",
        "# see: https://en.wikipedia.org/wiki/Discrete_Laplace_operator\n",
        "lapl.weight.data = torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]]).reshape(3, 3, 1, 1).float()\n",
        "\n",
        "# apply the operator and view the image\n",
        "cat_lapl = lapl(img)\n",
        "cat_lapl_py = transforms.ToPILImage()(cat_lapl[0])\n",
        "display(cat_lapl_py)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpSpEWrP_lDu"
      },
      "source": [
        "## Building a Convolutional Neural Network\n",
        "\n",
        "<img src='https://pythonmachinelearning.pro/wp-content/uploads/2017/09/lenet-5.png.webp'>\n",
        "\n",
        "Ref: LeCun Y, Bottou L, Bengio Y, Haffner P. <a href='https://ieeexplore.ieee.org/abstract/document/726791'>Gradient-based learning applied to document recognition</a>. Proceedings of the IEEE., 86(11), 2278-2324 (1998).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UK4vo1_5v1"
      },
      "source": [
        "## Example: Classifying Phases of Matter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3Wg7SF5cP5z"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltBPMAUET8yI"
      },
      "source": [
        "def load_dataset():\n",
        "\n",
        "  # download files\n",
        "  if not os.path.exists(\"Ising2DFM_reSample_L40_T=All.pkl\") or not os.path.exists(\"Ising2DFM_reSample_L40_T=All_labels.pkl\"):\n",
        "    os.system(\"wget https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/Ising2DFM_reSample_L40_T=All.pkl\")\n",
        "    os.system(\"wget https://physics.bu.edu/~pankajm/ML-Review-Datasets/isingMC/Ising2DFM_reSample_L40_T=All_labels.pkl\")\n",
        "\n",
        "  #X = pickle.load(open('Ising2DFM_reSample_L40_T=All.pkl', 'rb'))\n",
        "  # read data: 40x40 images, -1 = spin down, +1 = spin up\n",
        "  X = pd.read_pickle('Ising2DFM_reSample_L40_T=All.pkl')\n",
        "  X = np.unpackbits(X).reshape(-1, 1, 40, 40)\n",
        "  X = X.astype('int')\n",
        "  X[np.where(X==0)] = -1\n",
        "\n",
        "  #y = pickle.load(open('Ising2DFM_reSample_L40_T=All_labels.pkl', 'rb'))\n",
        "  # read labels: 0 = disordered, 1 = ordered\n",
        "  y = pd.read_pickle('Ising2DFM_reSample_L40_T=All_labels.pkl')\n",
        "\n",
        "  # Create ordered/disordered dataset\n",
        "  X_ordered=X[:70000,:]\n",
        "  y_ordered=y[:70000]\n",
        "  X_critical=X[70000:100000,:]\n",
        "  y_critical=y[70000:100000]\n",
        "  X_disordered=X[100000:,:]\n",
        "  y_disordered=y[100000:]\n",
        "  X = np.concatenate((X_ordered, X_disordered))\n",
        "  y = np.concatenate((y_ordered, y_disordered))\n",
        "\n",
        "  # pick random data points from ordered and disordered states to create the training and test sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
        "\n",
        "  return torch.tensor(X_train).float(), torch.tensor(X_test).float(), \\\n",
        "          torch.tensor(y_train), torch.tensor(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogOe4Kii_8l4"
      },
      "source": [
        "X_train, X_test, y_train, y_test = load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLECPUhqcS0r"
      },
      "source": [
        "### Plot some representative samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0lqgXwDED6A"
      },
      "source": [
        "f, axes = plt.subplots(1, 4)\n",
        "f.set_size_inches((16, 4))\n",
        "axes[0].imshow(X_train.squeeze().numpy()[0], vmin=-1, vmax=1)\n",
        "axes[1].imshow(X_train.squeeze().numpy()[1], vmin=-1, vmax=1)\n",
        "axes[2].imshow(X_train.squeeze().numpy()[2], vmin=-1, vmax=1)\n",
        "axes[3].imshow(X_train.squeeze().numpy()[3], vmin=-1, vmax=1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKkY4-XxcZJ6"
      },
      "source": [
        "### What happens when we apply `Conv2d()`? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCNm4zLcitkA"
      },
      "source": [
        "sample = X_train[1:2]\n",
        "conv = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5, stride=2)\n",
        "sample = F.pad(sample, (2, 2, 2, 2), 'circular').float()\n",
        "out = conv(sample)\n",
        "\n",
        "f, axes = plt.subplots(1, 2)\n",
        "f.set_size_inches((8, 4))\n",
        "axes[0].imshow(sample.squeeze().numpy(), vmin=-1, vmax=1)\n",
        "axes[1].imshow(out.detach().squeeze().numpy()[40], vmin=-1, vmax=1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3doyM3A_8dCA"
      },
      "source": [
        "### Build the Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJCsbrBNj1iW"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.DIM = 16\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.DIM, \\\n",
        "                               kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(self.DIM)\n",
        "        self.conv2 = nn.Conv2d(in_channels=self.DIM, out_channels=2*self.DIM, \\\n",
        "                               kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(2*self.DIM)\n",
        "        self.conv3 = nn.Conv2d(in_channels=2*self.DIM, out_channels=4*self.DIM, \\\n",
        "                               kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(4*self.DIM)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=5)\n",
        "        self.fc1 = nn.Linear(4*self.DIM, 2*self.DIM)\n",
        "        self.fc2 = nn.Linear(2*self.DIM, self.DIM)\n",
        "        self.fc3 = nn.Linear(self.DIM, 2)\n",
        "    \n",
        "    def pad(self, x):\n",
        "        # circular padding = periodic boundary condition\n",
        "        return F.pad(x, (2,2,2,2), 'circular')\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input: BATCH_SIZE x 1 x 40 x 40\n",
        "        # layer 1: BATCH_SIZE x DIM x 20 x 20        \n",
        "        out = F.relu(self.bn1(self.conv1(self.pad(x))))\n",
        "        # layer 2: BATCH_SIZE x 2*DIM x 10 x 10\n",
        "        out = F.relu(self.bn2(self.conv2(self.pad(out))))\n",
        "        # layer 3: BATCH_SIZE x 4*DIM x 5 x 5\n",
        "        out = F.relu(self.bn3(self.conv3(self.pad(out))))\n",
        "        # layer 4: BATCH_SIZE x 4*DIM\n",
        "        out = self.pool(out).reshape(-1, 4*self.DIM)\n",
        "        # layer 5: BATCH_SIZE x 2*DIM\n",
        "        out = F.dropout(F.relu(self.fc1(out)), p=0.2, training=self.training)\n",
        "        # layer 6: BATCH_SIZE x DIM\n",
        "        out = F.dropout(F.relu(self.fc2(out)), p=0.2, training=self.training)\n",
        "        # layer 7: BATCH_SIZE x 2\n",
        "        out = self.fc3(out)\n",
        "        return F.softmax(out, dim=1)\n",
        "\n",
        "    def return_activations(self, x):\n",
        "        activations = []\n",
        "        # input: BATCH_SIZE x 1 x 40 x 40\n",
        "        # layer 1: BATCH_SIZE x DIM x 20 x 20        \n",
        "        out = F.relu(self.bn1(self.conv1(self.pad(x))))\n",
        "        activations.append(out)\n",
        "        # layer 2: BATCH_SIZE x 2*DIM x 10 x 10\n",
        "        out = F.relu(self.bn2(self.conv2(self.pad(out))))\n",
        "        activations.append(out)\n",
        "        # layer 3: BATCH_SIZE x 4*DIM x 5 x 5\n",
        "        out = F.relu(self.bn3(self.conv3(self.pad(out))))\n",
        "        activations.append(out)\n",
        "        return activations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh61NaNduK0U"
      },
      "source": [
        "from skorch import NeuralNetClassifier\n",
        "model = Net()\n",
        "clf = NeuralNetClassifier(model, batch_size=64, max_epochs=2, lr=1e-3, device='cuda')\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdEV4IwEuyQL"
      },
      "source": [
        "clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17vZzSyI8hgs"
      },
      "source": [
        "### Visualizing the layer-by-layer activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8fS8rQixYZ-"
      },
      "source": [
        "i = 1\n",
        "example = X_train[i:i+1].to('cuda')\n",
        "print(example.shape)\n",
        "activations = clf.module_.return_activations(example)\n",
        "f, axes = plt.subplots(1, 4)\n",
        "f.set_size_inches((16, 4))\n",
        "axes[0].imshow(example.cpu().squeeze().numpy(), vmin=-1, vmax=1)\n",
        "axes[1].imshow(activations[0][0, np.random.randint(16), :, :].detach().cpu().squeeze().numpy(), vmin=-1, vmax=1)\n",
        "axes[2].imshow(activations[1][0, np.random.randint(32), :, :].detach().cpu().squeeze().numpy(), vmin=-1, vmax=1)\n",
        "axes[3].imshow(activations[2][0, np.random.randint(64), :, :].detach().cpu().squeeze().numpy(), vmin=-1, vmax=1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}