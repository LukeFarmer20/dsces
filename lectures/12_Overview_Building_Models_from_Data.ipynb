{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "12_Overview_Building_Models_from_Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNik9Rnf32B2"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, make_scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VB0Jy4RF1KV"
      },
      "source": [
        "# Overview: Building Models from Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMUqxQrjF6gk"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81OFjccJ7ELn"
      },
      "source": [
        "os.system(\"wget https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/delaney-processed.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s3j5aWAFilX"
      },
      "source": [
        "ESOL (delaney) is a standard regression dataset containing structures and water solubility data for 1128 compounds. The dataset is widely used to validate machine learning models on estimating solubility directly from molecular structures (as encoded in SMILES strings).\n",
        "\n",
        "Reference:\n",
        "\n",
        "Delaney, John S. \"ESOL: estimating aqueous solubility directly from molecular structure.\" Journal of chemical information and computer sciences 44.3 (2004): 1000-1005."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV1ddHRWCMo7"
      },
      "source": [
        "with open('delaney-processed.csv') as f:\n",
        "  data = f.readlines()\n",
        "\n",
        "print(\"\".join(data[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDUh8C8ZGCB_"
      },
      "source": [
        "## Working with DataFrames\n",
        "\n",
        "Rather than parsing the data manual, we can use a dedicated function to load the .csv file into a pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2GOuG9T7zjO"
      },
      "source": [
        "df = pd.read_csv(\"delaney-processed.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nULB8WBGvUH"
      },
      "source": [
        "## Constructing the feature/label arrays\n",
        "\n",
        "In order to build a data-driven model, we must select the task we'd like to predict (labels, `y`) and the information about each example to give to the model (features, `X`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpouNg7673MX"
      },
      "source": [
        "features = df[df.keys()[2:8]]\n",
        "features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEprrOfn8Hk8"
      },
      "source": [
        "label = df[df.keys()[8]]\n",
        "label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQiZVEib8Pwc"
      },
      "source": [
        "X = features.values\n",
        "y = label.values\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui_GUIDEMCLU"
      },
      "source": [
        "## Data visualization\n",
        "\n",
        "Let's take a look at the distribution of solubility values we'd like to predict. This is a good first step to take when building a model, it allows you to visually identify some data anomalies or outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECyI0_SiMIkc"
      },
      "source": [
        "plt.hist(y, 50)\n",
        "plt.xlabel(\"Solubility\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EctQyyO4IT34"
      },
      "source": [
        "## Data splitting\n",
        "\n",
        "In order to test the performance of our model on unseen data, we split the dataset into two parts: training set (80%) and test set (20%)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE3NyTFJ8xue"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifBeIOUxIea4"
      },
      "source": [
        "## Model: Linear regression\n",
        "\n",
        "For this example, we choose the simplest data-driven model: linear regression. In this model, we determine a set of variables $w$ such that the quantity $y = X w$ is minimized.\n",
        "\n",
        "$$ \\min_w y - Xw $$\n",
        "\n",
        "We minimize the square of the cost function above using the training set. This amounts to performing a linear least-squares minimization to determine $w$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb-Q5E7X9P1J"
      },
      "source": [
        "reg = LinearRegression().fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d3hheTXKtIk"
      },
      "source": [
        "## Model evaluation: Coefficient of determination ($R^2$ score)\n",
        "\n",
        "Now that we've trained out model, we'd like to evaluate the performance of the model on the training set as well as the test set. There are many metrics available to evaluate the performance of your model. In this example, we choose the coefficient of determination or R2 score. The R2 score is calculated as:\n",
        "\n",
        "$$ R^2 = 1 - \\frac{\\sum_i (y_{true} - y_{pred})^2}{\\sum_i (y_{true} - \\bar{y})^2} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtDOp6D7-xM2"
      },
      "source": [
        "train_pred = reg.predict(X_train)\n",
        "test_pred = reg.predict(X_test)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_pred)\n",
        "test_r2 = r2_score(y_test, test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHZL7XgrMr-y"
      },
      "source": [
        "## Model evaluation: Plotting\n",
        "\n",
        "Let's construct a parity plot to display our model predictions and their true values. A parity plot is constructed by scattering the experimental (true) values along the x-axis and the predicted values on the y-axis. We can construct a parity plot for the train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lqu5c8XI_qQm"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(15, 7.5))\n",
        "ax[0].scatter(y_train, train_pred)\n",
        "ax[0].plot(np.linspace(-10, 2), np.linspace(-10, 2), 'r')\n",
        "ax[0].set_xlim((-10, 2))\n",
        "ax[0].set_ylim((-10, 2))\n",
        "ax[0].set_aspect('equal', adjustable='box')\n",
        "ax[0].set_xlabel(\"Log Solubility\")\n",
        "ax[0].set_ylabel(\"Predicted Log Solubility\")\n",
        "ax[0].set_title(\"Train R2: {}\".format(r2_score(y_train, train_pred)))\n",
        "ax[1].scatter(y_test, test_pred)\n",
        "ax[1].plot(np.linspace(-10, 2), np.linspace(-10, 2), 'r')\n",
        "ax[1].set_xlim((-10, 2))\n",
        "ax[1].set_ylim((-10, 2))\n",
        "ax[1].set_aspect('equal', adjustable='box')\n",
        "ax[1].set_xlabel(\"Log Solubility\")\n",
        "ax[1].set_ylabel(\"Predicted Log Solubility\")\n",
        "ax[1].set_title(\"Test R2: {}\".format(r2_score(y_test, test_pred)))\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO4c0mwFM_-d"
      },
      "source": [
        "## Interpreting our model\n",
        "\n",
        "Linear regression models have a much simpler functional form compared to our machine learning models we will use later on in this course. This simple functional form, however, allows us to easily interpret the model by looking at the coefficients $w$. In this case, each parameter $w_i$ tells us how important each feature $i$ is (magnitude) and how the model uses each feature (sign)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8teGdrk_9W9"
      },
      "source": [
        "reg.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0Sa1ynMAIH6"
      },
      "source": [
        "list(zip(df.keys()[2:8], reg.coef_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMZkbO8nNs-E"
      },
      "source": [
        "## Linear regression the hard (?) way\n",
        "\n",
        "We've already learned how to solve an over-determined linear system of equations using linear least-squares via the `np.linalg.lstsq` function. Let's compare our results with the `sklearn` model we just constructed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWAsWxk0AQWj"
      },
      "source": [
        "x, _, _, _ = np.linalg.lstsq(X_train, y_train, rcond=-1)\n",
        "list(zip(df.keys()[2:8], x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUJrfFbNAdRK"
      },
      "source": [
        "train_pred = X_train @ x\n",
        "test_pred = X_test @ x\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 7.5))\n",
        "ax[0].scatter(y_train, train_pred)\n",
        "ax[0].plot(np.linspace(-10, 2), np.linspace(-10, 2), 'r')\n",
        "ax[0].set_xlim((-10, 2))\n",
        "ax[0].set_ylim((-10, 2))\n",
        "ax[0].set_aspect('equal', adjustable='box')\n",
        "ax[0].set_xlabel(\"Log Solubility\")\n",
        "ax[0].set_ylabel(\"Predicted Log Solubility\")\n",
        "ax[0].set_title(\"Train R2: {}\".format(r2_score(y_train, train_pred)))\n",
        "ax[1].scatter(y_test, test_pred)\n",
        "ax[1].plot(np.linspace(-10, 2), np.linspace(-10, 2), 'r')\n",
        "ax[1].set_xlim((-10, 2))\n",
        "ax[1].set_ylim((-10, 2))\n",
        "ax[1].set_aspect('equal', adjustable='box')\n",
        "ax[1].set_xlabel(\"Log Solubility\")\n",
        "ax[1].set_ylabel(\"Predicted Log Solubility\")\n",
        "ax[1].set_title(\"Test R2: {}\".format(r2_score(y_test, test_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqS-BtHaOSwt"
      },
      "source": [
        "## Model evaluation: Cross-validation\n",
        "\n",
        "Cross-validation is another technique for evaluating the performance of a model. The dataset is split into $k$ equal size partitions or folds. The model is then trained on $k-1$ folds and tested on the $k$-th fold. This procedure is repeated $k$ times. Model performance and evaluation can vary from run to run based on differences in how our original dataset is randomly partitioned into training and test sets. By evaluating the model multiple times with different training and test sets, we can estimate the error in model evaluation that can arise from dataset splitting differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uDiptxY8fX6"
      },
      "source": [
        "scoring = {'r2_score': make_scorer(r2_score)}\n",
        "cv = cross_validate(LinearRegression(), X, y, scoring=scoring, return_train_score=True)\n",
        "cv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BARv51GoA6tn"
      },
      "source": [
        "for score in ['train_r2_score', 'test_r2_score']:\n",
        "  scores = cv[score]\n",
        "  mean_scores = np.round(np.mean(scores), 4)\n",
        "  stderr_scores = np.round(stats.sem(scores), 4)\n",
        "  print(\"{}: {} +/- {}\".format(score, mean_scores, stderr_scores))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}