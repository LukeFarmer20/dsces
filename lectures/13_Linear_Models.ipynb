{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "13_Linear_Models.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPI0jnhGygoC"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRFwrtvRxe2i"
      },
      "source": [
        "# Linear Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcE0yTNSxkxm"
      },
      "source": [
        "## Dataset: BACE-1\n",
        "\n",
        "Beta-Secretase 1 (BACE) is a transmembrane aspartic-acid protease human protein encoded by the BACE1 gene. BACE is essential for the generation of beta-amyloid peptide in neural tissue, a component of amyloid plaques widely believed to be critical in the development of Alzheimer's, rendering BACE an attractive therapeutic target. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlfLtftFxdvn"
      },
      "source": [
        "os.system(\"wget https://raw.githubusercontent.com/deepchem/deepchem/master/datasets/desc_canvas_aug30.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh8CgCrc2M2_"
      },
      "source": [
        "This dataset contains a set of molecular structures `mol`, half-maximal inhibitory concentration `pIC50`, and 590 molecular topological features. These features can be calculated using common chemistry Python packages like `openbabel` or `rdkit`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7se9fPzzCv4"
      },
      "source": [
        "df = pd.read_csv(\"desc_canvas_aug30.csv\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWmETP_d24Tc"
      },
      "source": [
        "This dataset was previous used in a drug design competition sponsored by Novartis. Here, we use the original train/test dataset splitting of the contest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_37WS53w0kvk"
      },
      "source": [
        "train_df = df[df['Model'] == \"Train\"]\n",
        "test_df = df[df['Model'] == \"Test\"]\n",
        "\n",
        "label = 'pIC50'\n",
        "y_train = train_df[label].values\n",
        "y_test = test_df[label].values\n",
        "\n",
        "features = list(train_df.keys()[5:-1])\n",
        "features = [f for f in features if not np.isnan(np.sum(train_df[f].values))]\n",
        "X_train = train_df[features].values\n",
        "X_test = test_df[features].values\n",
        "\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNzdEIrJ3Kua"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "In linear regression, we fit a model of the form:\n",
        "\n",
        "$$ y(\\textbf{x}, \\textbf{w}) = w_0 + \\sum_{j=1}^{M-1} w_j \\phi_j(\\textbf{x}) $$\n",
        "\n",
        "where $\\phi_j(\\textbf{x})$ are known as basis functions and M is the total number of parameters in this model.\n",
        "\n",
        "The parameters $w_0$ allows for any fixed offset in the data and is sometimes called the bias paramaters. We can introduce a dummy basis function $\\phi_0(\\textbf{x}) = 1$ so that\n",
        "\n",
        "$$ y(\\textbf{x}, \\textbf{w}) = \\sum_{j=0}^{M-1} w_j \\phi_j(\\textbf{x}) = \\textbf{w}^T \\boldsymbol{\\phi}(\\textbf{x}) $$\n",
        "\n",
        "The objective function\n",
        "\n",
        "$$ E_D(\\textbf{w}) = \\frac{1}{2} \\sum_{n=1}^N [t_n - \\textbf{w}^T \\boldsymbol{\\phi}(\\textbf{x}_n)]^2$$\n",
        "\n",
        "is minimized via an Ordinary Least Squares optimization over N labeled examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbikda0O_Q4M"
      },
      "source": [
        "reg = LinearRegression()\n",
        "reg.fit(X_train, y_train)\n",
        "print(reg.score(X_train, y_train))\n",
        "print(reg.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsMQsmHb3ODI"
      },
      "source": [
        "## Lasso (L1-regularized) Regression\n",
        "\n",
        "In Lasso regression, we modify the objective function to include an L1-norm penalty on the weights of the model. This procedure is designed to reduce overfitting by constraining parameters from taking extreme values. This process is known as regularization, or adding information or constraints to prevent overfitting. This works by limiting model complexity by enforcing sparsity within the parameter set. Regularization is an important concept in all machine learning-based models.\n",
        "\n",
        "The new objective function becomes\n",
        "\n",
        "$$ E_D(\\textbf{w}) = \\frac{1}{2} \\sum_{n=1}^N [t_n - \\textbf{w}^T \\boldsymbol{\\phi}(\\textbf{x}_n)]^2 + \\frac{\\alpha}{2} \\sum_{j=0}^{M-1} |w_j|$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1qFX_gn_ssX"
      },
      "source": [
        "reg = Lasso(alpha=0.1)\n",
        "reg.fit(X_train, y_train)\n",
        "print(reg.score(X_train, y_train))\n",
        "print(reg.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvXkWTC4D9Uq"
      },
      "source": [
        "## Exercise: Tune alpha\n",
        "\n",
        "Try adjusting alpha to maximize the score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFV1VOVkEBMA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xYg301a3To6"
      },
      "source": [
        "## Ridge (L2-regularized) Regression\n",
        "\n",
        "In Lasso regression, we modify the objective function to include an L2-norm penalty on the weights of the model. The L2-regularization typically results in a less-sparse set of weights compared with L1-regularization.\n",
        "\n",
        "The new objective function becomes\n",
        "\n",
        "$$ E_D(\\textbf{w}) = \\frac{1}{2} \\sum_{n=1}^N [t_n - \\textbf{w}^T \\boldsymbol{\\phi}(\\textbf{x}_n)]^2 + \\frac{\\alpha}{2} \\sum_{j=0}^{M-1} w_j^2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_sBwuOICpjH"
      },
      "source": [
        "reg = Ridge(alpha=0.1)\n",
        "reg.fit(X_train, y_train)\n",
        "print(reg.score(X_train, y_train))\n",
        "print(reg.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YCO88d6Gbhe"
      },
      "source": [
        "## Exercise: Tune alpha\n",
        "\n",
        "Try adjusting alpha to maximize the score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlsr0Hn_GbSj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}