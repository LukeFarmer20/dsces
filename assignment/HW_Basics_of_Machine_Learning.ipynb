{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW - Basics of Machine Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0D8WvtyyUUw"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UD4SO2EHSNMk"
      },
      "source": [
        "# Homework: Basics of Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD_q7xSkRbQp"
      },
      "source": [
        "### 3 Phase Oil Dataset\n",
        "\n",
        "Description (<a href=\"https://inverseprobability.com/3PhaseData\">source</a>)\n",
        "\n",
        "This is synthetic data modelling non-intrusive measurements on a pipe-line transporting a mixture of oil, water and gas. The flow in the pipe takes one out of three possible configurations: horizontally stratified, nested annular or homogeneous mixture flow. The data lives in a 12-dimensional measurement space, but for each configuration, there is only two degrees of freedom: the fraction of water and the fraction of oil. (The fraction of gas is redundant, since the three fractions must sum to one.) Hence, the data lives on a number of ‘sheets’ which locally are approximately 2-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poBwav1FOE5c"
      },
      "source": [
        "os.system(\"wget http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/resources/3PhData.tar.gz\")\n",
        "os.system(\"tar -xvzf 3PhData.tar.gz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvcVutNsORYs"
      },
      "source": [
        "# Load training data\n",
        "X_train = np.loadtxt('DataTrn.txt')\n",
        "X_train_frac = np.loadtxt('DataTrnFrctns.txt')\n",
        "y_train = np.loadtxt('DataTrnLbls.txt')\n",
        "y_train = np.argmax(y_train, axis=1)\n",
        "\n",
        "# Load validation data\n",
        "X_valid = np.loadtxt('DataVdn.txt')\n",
        "X_valid_frac = np.loadtxt('DataVdnFrctns.txt')\n",
        "y_valid = np.loadtxt('DataVdnLbls.txt')\n",
        "y_valid = np.argmax(y_valid, axis=1)\n",
        "\n",
        "# Load test data\n",
        "X_test = np.loadtxt('DataTst.txt')\n",
        "X_test_frac = np.loadtxt('DataTstFrctns.txt')\n",
        "y_test = np.loadtxt('DataTstLbls.txt')\n",
        "y_test = np.argmax(y_test, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3bYH1YfSVD7"
      },
      "source": [
        "## Question 1: Data Visualization\n",
        "\n",
        "Examine the distribution (see: histogram) of features and labels in the training and validation datasets. For each feature and label, comment on whether imbalances in the distribution of values (is the distribution binomial? does the distribution span multiple orders of magnitude?) exist as well as any difference between the training data distribution and validation data distribution. For each plot, be sure to label and title each plot appropriately.\n",
        "\n",
        "Example:\n",
        "\n",
        "```\n",
        "# plot histogram of first feature in training datasets\n",
        "plt.hist(X_train[:, 0])\n",
        "plt.title(\"X_train Feature: 1\")\n",
        "plt.xlabel(\"X_1\")\n",
        "plt.ylabel(\"Unnormalized Counts\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLmfgorKnynu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrriSrVTSVIR"
      },
      "source": [
        "## Question 2: Train a Linear Classification Model using the Measurements\n",
        "\n",
        "Train a logistic regression (e.g. linear classifier) using the training dataset. Build three models:\n",
        "\n",
        "1. No regularization applied\n",
        "2. L1-regularization applied\n",
        "3. L2-regularization applied\n",
        "\n",
        "For models with regularization, adjust the regularization strength to maximize performance on the validation set. Evaluate the generalization of each final model with the test dataset. It is not necessary to show the performance of all models trained, just evaluate the best 3 models you find as defined by validation set performance.\n",
        "\n",
        "To get started:\n",
        "```\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# No regularization\n",
        "clf = LogisticRegression(penalty='none')\n",
        "# L1 regularization, lambda = C**-1 = 0.01\n",
        "clf_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=100.)\n",
        "# L2 regularization, lambda = C**-1 = 0.01\n",
        "clf_l2 = LogisticRegression(penalty='l2', solver='liblinear', C=100.)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BihsRdinzLj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mriFu9YDSVRO"
      },
      "source": [
        "## Question 3: Train 2-Feature Model\n",
        "\n",
        "As mentioned in the data description, the phase of the oil/water mixture is uniquely determined by mol fractions of water and oil present in the mixture. Train a linear classifier using `X_train_frac` instead of `X_train` and evaluate the train, validation, and test scores. Due to the few new number of features in this example, applying regularization is not strictly necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUuQhE_Bnz6a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34LUOupoSVVX"
      },
      "source": [
        "## Question 4: Model Evaluation and Interpretation\n",
        "\n",
        "Why are we unable to train a linear classifier to correctly identify the phase of the 3-component mixture even though we have the minimum required amount of information from a thermodynamic viewpoint to exactly specify phase and composition? If you fell stuck here, try running the function `plot_2d_decision_boundary` to inspire your thinking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK7gUloS0No8"
      },
      "source": [
        "def plot_2d_decision_boundary():\n",
        "\n",
        "  # adapted from: \n",
        "  # https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "  clf = LogisticRegression()\n",
        "  clf.fit(X_train_frac, y_train)\n",
        "  X = X_train_frac\n",
        "  Y = y_train\n",
        "  # Plot the decision boundary. For that, we will assign a color to each\n",
        "  # point in the mesh [x_min, x_max] * [y_min, y_max].\n",
        "  # Here, x is mol fraction 1 and y is mol fraction 2\n",
        "  x_min, x_max = X[:, 0].min() - .05, X[:, 0].max() + .05\n",
        "  y_min, y_max = X[:, 1].min() - .05, X[:, 1].max() + .05\n",
        "  h = .02  # step size in the mesh\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "  Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "  # Put the result into a color plot\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  plt.figure(1, figsize=(4, 3))\n",
        "  plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "  # Plot also the training points\n",
        "  # Each class (Stratified, Annular, Homogeneous) is assigned a color\n",
        "  # that matches the corresponding decision area\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\n",
        "  plt.xlabel('mol fraction 1')\n",
        "  plt.ylabel('mol fraction 2')\n",
        "  plt.xlim(xx.min(), xx.max())\n",
        "  plt.ylim(yy.min(), yy.max())\n",
        "  plt.xticks(())\n",
        "  plt.yticks(())\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_2d_decision_boundary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}